{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimization methods\n",
    "Optimization methods attempt to find the minimum or maximum of functions. Here the steepest descent, conjugate gradient, Newton-Raphson, and Broyden-Fletcher-Goldfarb-Shanno methods are presented.\n",
    "\n",
    "Functions with two variables can be visualized as a surface (i.e., a contour map) where the lowest and highest point on the map corresponds to the function’s max and min.  Although higher dimensional functions cannot be readily visualized, the analogy of traversing a surface is still convenient. Using the map concept, one can appreciate the general features of these algorithms. \n",
    "\n",
    "1.\tSelect an initial guess, which is a point on the surface\n",
    "2.\tDetermine a direction to move\n",
    "3.\tTake a step by moving along the determined direction\n",
    "4.\tTest for convergence (Is the difference between the function’s values for the last two points less than a threshold value?)\n",
    "5.\tRepeat steps 2 to 4 to continue searching surface\n",
    "\n",
    "\n",
    "The most significant difference in the algorithms presented in this notebook is how each defines the step direction. The various approaches are described in the following sections.  It is important to emphasize that numerical optimizations can be extremely time consuming.  For the optimization to complete in a reasonable amount of time, the threshold for the convergence criteria must be selected with care.  In addition, an optimization algorithm should always limit the number of iterations so as not to get stuck in an endless search.  Lastly, the method should keep track of the step size because an indication that the search is proceeding successfully is a decrease in successive step sizes.\n",
    "\n",
    "# 5.1 Steepest Descent\n",
    "\n",
    "The steepest descent method is the simplest iterative method of optimization method. Steepest descent makes iterative steps in the direction of greatest decrease. A function’s gradient at a point determines the direction where the rate of changes is the highest.  Thus the opposite direction will be the steepest decent.\n",
    "\n",
    "For an initial guess, $x_0$, the search direction, $sd_0$, that is in the direction of steepest descent becomes\n",
    "\n",
    " $$ sd_0 = -\\nabla f(x_0)$$\n",
    "\n",
    "A step is taken by using a line search minimization along this search direction, \n",
    "\n",
    "$$ x_i = x_0 + \\lambda sd_0 $$\n",
    "\n",
    "where $\\lambda$ minimizes $f(x)$ in the search direction. For simplicity, the details of the of the minimization are ignored.\n",
    "\n",
    "A new search direction is constructed as the direction of steepest descent at the new point.\n",
    "\n",
    " $$ sd_i = -\\nabla f(x_i)$$\n",
    " \n",
    " \n",
    "As an example, the cell directly below implements steepest descent on the function,\n",
    " $$ f(x,y) = sin(0.5x^2-0.25y^2+3)*cos(2x+1-e^y)$$\n",
    "Following the code is an image depicting steepest descent implemented on the surface of this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "import numpy as np\n",
    "from scipy.optimize import line_search as linesearch\n",
    "import scipy.linalg as spla\n",
    "import scipy as sp\n",
    "import plotting_functions as pf\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as syp\n",
    "%matplotlib notebook\n",
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "\n",
    "def f2(x, y):\n",
    "    return syp.sin(0.5 * x**2 - 0.25 * y**2 + 3) * syp.cos(2 * x + 1 - syp.exp(y))\n",
    "\n",
    "\n",
    "\n",
    "Gradient = pf.evaluate_gradient(f2)\n",
    "hessian = pf.evaluate_hessian(f2)\n",
    "# Hessian = compact_input(hessian)\n",
    "f = pf.compact_input(f2)\n",
    "\n",
    "##########\n",
    "# GLOBAL VARS\n",
    "##########\n",
    "# initial guess\n",
    "initial_guess = np.array((1.6, -1.5))  \n",
    "print(initial_guess)\n",
    "f(initial_guess)\n",
    "# convergence criteria\n",
    "residual_tolerance = 1e-6  \n",
    "update_tolerance = 1e-5\n",
    "IT_MAX = 1000\n",
    "# domain for plotting\n",
    "xminimum = -1\n",
    "xmaximum = 2\n",
    "yminimum = -4\n",
    "ymaximum = 1\n",
    "#data structures to summarize resules\n",
    "steps_per_method = dict()\n",
    "difference_per_method = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initial_guess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-32967ed5ed9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#MAIN CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m##########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mold_guess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_guess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msearch_direction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_guess\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set the search direction to the negative gradient at the initial guess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconverged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initial_guess' is not defined"
     ]
    }
   ],
   "source": [
    "##########\n",
    "#MAIN CODE\n",
    "##########\n",
    "old_guess = initial_guess\n",
    "search_direction = -Gradient(old_guess) # set the search direction to the negative gradient at the initial guess\n",
    "converged = False \n",
    "num_steps = 0\n",
    "steps_x =[]\n",
    "steps_y =[]\n",
    "steps_x.append(old_guess[0])\n",
    "steps_y.append(old_guess[1])\n",
    "#repeat until convergenced or exceed IT_MAX\n",
    "while not converged and num_steps <= IT_MAX: \n",
    "    print(\"STEP {:d}\".format(num_steps))\n",
    "    num_steps+=1\n",
    "    gamma = linesearch(f,Gradient,old_guess,search_direction)[0] # find the step size that minimizes f along the search direction\n",
    "    if gamma == None: \n",
    "        print(\"ERROR: No appropriate step size found. Exiting minimization\")\n",
    "        exit()\n",
    "    else:\n",
    "        new_guess = old_guess + gamma*search_direction # update the current guess\n",
    "        search_direction = -Gradient(new_guess) # set the new search direction to the negative gradient at the new guess\n",
    "        diff = old_guess-new_guess # calculate the vector difference of the old guess and new guess\n",
    "        diff = np.sqrt(np.dot(diff,diff)) # calculate the magnitude of the separation between the old guess and the new guess\n",
    "        print(\"OLD GUESS: {}\".format(old_guess))\n",
    "        print(\"NEW GUESS: {}\".format(new_guess))\n",
    "        print(\"DIFFERENCE: {}\".format(diff))\n",
    "        old_guess = new_guess # update the old guess so we can take another step\n",
    "        steps_x.append(old_guess[0])\n",
    "        steps_y.append(old_guess[1])\n",
    "        if(diff<residual_tolerance): # if the step was small enough consider this the minimum\n",
    "            converged=True\n",
    "steps_x = np.array(steps_x)\n",
    "steps_y = np.array(steps_y)\n",
    "steps_per_method[\"Steepest Descent\"] = num_steps\n",
    "difference_per_method[\"Steepest Descent\"] = diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot_func(xminimum, xmaximum, yminimum, ymaximum, steps_x, steps_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Conjugate Gradient #\n",
    "\n",
    "Conjugate gradient is very similar to the  steepest descent method, however the method attempts to ensure previous progress is not undone with future search steps.\n",
    "The added restriction is called conjugacy.\n",
    "Again, for an initial guess, $x_0$ the starting search direction, $sd_0$ is taken as the negative gradient.\n",
    "\n",
    " $$ sd_0 = -\\nabla f(x_0)$$\n",
    "\n",
    "The line search minimization along this search direction provides the step size.\n",
    "All new search directions are constructed by including a term to prevent successive steps from diminishing progress made from the previous search direction.\n",
    "This second term is determined through a process similar to orthogonalization.\n",
    "\n",
    "$$ sd_{i+1} = -\\nabla f(x_{i+1}) + \\frac{(-\\nabla f(x_{i+1})) \\bullet (-\\nabla f(x_{i+1})) }{(-\\nabla f(x_{i})) \\bullet (-\\nabla f(x_{i}))} sd_{i} $$\n",
    "\n",
    "In practice, the line search minimization may fail because the search direction may point uphill.\n",
    "This can be resolved by resetting the search direction to point in the direction of steepest descent.\n",
    "\n",
    "The code below applies the conjugate gradient method to the example function and plots the search path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# MAIN CODE #\n",
    "#############\n",
    "converged = False\n",
    "num_steps = 0\n",
    "old_guess = initial_guess\n",
    "old_grad = -Gradient(old_guess)\n",
    "# set first search direction in the direction of steepest descent\n",
    "old_search_direction = old_grad\n",
    "steps_x = []\n",
    "steps_y = []\n",
    "steps_x.append(old_guess[0])\n",
    "steps_y.append(old_guess[1])\n",
    "while not converged and num_steps <= IT_MAX:\n",
    "    print(\"STEP {:d}\".format(num_steps))\n",
    "    alpha = linesearch(f, Gradient, old_guess, old_search_direction)[0]\n",
    "    if alpha == None:\n",
    "        print(\"WARNING: LINE SEARCH DID NOT CONVERGE... RESETTING SEARCH DIRECTION TO STEEPEST DESCENT!\")\n",
    "        old_search_direction = -Gradient(old_guess)\n",
    "        alpha = linesearch(f, Gradient, old_guess, old_search_direction)[0]  # step size\n",
    "    new_guess = old_guess + alpha * old_search_direction\n",
    "    new_grad = -Gradient(new_guess)\n",
    "    # gamma encompasses the fraction in the above equation\n",
    "    gamma = np.dot(new_grad - old_grad, new_grad) / np.dot(old_grad, old_grad)\n",
    "    # update the search direction\n",
    "    new_search_direction = new_grad + np.dot(gamma, old_search_direction)\n",
    "    diff = old_guess - new_guess\n",
    "    diff = np.sqrt(np.dot(diff, diff))\n",
    "    print(\"OLD GUESS: {}\".format(old_guess))\n",
    "    print(\"NEW GUESS: {}\".format(new_guess))\n",
    "    print(\"DIFFERENCE: {}\".format(diff))\n",
    "    num_steps += 1\n",
    "    old_guess = new_guess\n",
    "    old_grad = new_grad\n",
    "    old_search_direction = new_search_direction\n",
    "    steps_x.append(old_guess[0])\n",
    "    steps_y.append(old_guess[1])\n",
    "    if(diff < residual_tolerance):\n",
    "        converged = True\n",
    "steps_x = np.array(steps_x)\n",
    "steps_y = np.array(steps_y)\n",
    "steps_per_method[\"Conjugate Gradient\"] = num_steps\n",
    "difference_per_method[\"Conjugate Gradient\"] = diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.plot_func(xminimum, xmaximum, yminimum, ymaximum, steps_x, steps_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Newton-Raphson\n",
    "\n",
    "In the root-finding section of these lessons, the Newton-Raphson root finding method was described for a one-dimensional system.\n",
    "This method can be extended to higher dimensions. As a reminder, the Newton-Raphson method takes iterative steps:\n",
    "\n",
    "$$x_{i+1}\\ =\\ x_i - \\frac{f(x_i)}{f'(x_i)}$$\n",
    "\n",
    "To minimize the first derivative to find an extremum, the following iterative equation could be used:\n",
    "$$x_{i+1}\\ =\\ x_i - \\frac{f'(x_i)}{f''(x_i)}$$\n",
    "\n",
    "To optimize a higher order function, the Hessian matrix is introduced.\n",
    "The Hessian is a square matrix composed of second derivatives that describe the local curvature of a function. The iterative steps are given by:\n",
    "$$ p_i = (x_i,y_i) $$\n",
    "and\n",
    "$$p_{i+1} = p_i - \\gamma[H(p_i)]^{-1}]\\nabla f(p_i)$$\n",
    "\n",
    "where the step size, $\\gamma$, is found using a line search and $H(p_i)$ is the Hessian:\n",
    "$$H(p_i)= \\ \\begin{bmatrix}\n",
    "{\\frac{\\partial^2 f(x_i,y_i)}{\\partial x^2}} \n",
    "& {\\frac{\\partial^2 f(x_i,y_i)}{\\partial x \\partial y}} \\\\\n",
    "{\\frac{\\partial^2 f(x_i,y_i)}{\\partial y \\partial x}} \n",
    "& {\\frac{\\partial^2 f(x_i,y_i)}{\\partial y^2}} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "\n",
    "It is important to note that the Newton-Raphson method requires the Hessian to be positive definite, which ensures a local minimum near $p_i$ is isolated and not a saddle point or local maximum.\n",
    "We will see below that the Newton-Raphson method finds the minimum of a well-behaved, convex function, such as $x^2\\ +\\ y^2$. If we solve for the Hessian of this function:\n",
    "$$Hf(x,y)= \\ \\begin{bmatrix}\n",
    "{\\frac{\\partial^2 f(x,y)}{\\partial x^2}} \n",
    "& {\\frac{\\partial^2 f(x,y)}{\\partial x \\partial y}} \\\\\n",
    "{\\frac{\\partial^2 f(x,y)}{\\partial y \\partial x}} \n",
    "& {\\frac{\\partial^2 f(x,y)}{\\partial y^2}} \n",
    "\\end{bmatrix} $$\n",
    "$$Hf(x,y)= \\ \\begin{bmatrix}\n",
    "{\\frac{\\partial^2 (x^2+y^2)}{\\partial x^2}} \n",
    "& {\\frac{\\partial^2 (x^2+y^2)}{\\partial x \\partial y}} \\\\\n",
    "{\\frac{\\partial^2 (x^2+y^2)}{\\partial y \\partial x}} \n",
    "& {\\frac{\\partial^2 (x^2+y^2)}{\\partial y^2}} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$Hf(x,y)= \\ \\begin{bmatrix}\n",
    "2\n",
    "& 0 \\\\\n",
    "0 \n",
    "& 2 \n",
    "\\end{bmatrix} $$\n",
    "These positive values of the Hessian indicate that our function has a local minimum, thus Newton's method of optimization will perform well.\n",
    "Despite the requirement of a positive definite Hessian, the Newton-Raphson's method has excellent convergence since most functions near a minimum resemble $x^2\\ +\\ y^2$.\n",
    "\n",
    "The code below applies the Newton-Raphson optimization method to the example function and plots the optimization path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# MAIN CODE\n",
    "##########\n",
    "old_guess = initial_guess\n",
    "old_search_direction = -Gradient(old_guess)\n",
    "num_steps = 0\n",
    "converged = False\n",
    "steps_x = []\n",
    "steps_y = []\n",
    "steps_x.append(old_guess[0])\n",
    "steps_y.append(old_guess[1])\n",
    "while not converged and num_steps <= IT_MAX:\n",
    "    print(\"STEP {:d}\".format(num_steps))\n",
    "    num_steps += 1\n",
    "    A = Hessian(old_guess)\n",
    "    try:\n",
    "        if num_steps == 1:\n",
    "            new_search_direction = -Gradient(old_guess)\n",
    "            gamma = linesearch(\n",
    "                f, Gradient, old_guess, new_search_direction)[0]  # step size\n",
    "            new_guess = old_guess + gamma * new_search_direction\n",
    "        else:\n",
    "            new_search_direction = spla.solve(A, -Gradient(old_guess))\n",
    "            new_guess = old_guess + new_search_direction\n",
    "    except:\n",
    "        print(\"WARNING: HESSIAN IS NOT POSITIVE DEFINITE... RESETTING SEARCH DIRECTION TO STEEPEST DESCENT!\")\n",
    "        new_search_direction = -Gradient(old_guess)\n",
    "        gamma = linesearch(f, Gradient, old_guess, new_search_direction)[\n",
    "            0]  # step size\n",
    "        new_guess = old_guess + gamma * new_search_direction\n",
    "    diff = old_guess - new_guess\n",
    "    diff = np.sqrt(np.dot(diff, diff))\n",
    "    print(\"OLD GUESS: {}\".format(old_guess))\n",
    "    print(\"NEW GUESS: {}\".format(new_guess))\n",
    "    print(\"DIFFERENCE: {}\".format(diff))\n",
    "    old_guess = new_guess\n",
    "    steps_x.append(old_guess[0])\n",
    "    steps_y.append(old_guess[1])\n",
    "    old_search_direction = new_search_direction\n",
    "    if(diff < residual_tolerance):\n",
    "        converged = True\n",
    "steps_x = np.array(steps_x)\n",
    "steps_y = np.array(steps_y)\n",
    "steps_per_method[\"Newton-Raphson\"] = num_steps\n",
    "difference_per_method[\"Newton-Raphson\"] = diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pf.plot_func(xminimum, xmaximum, yminimum, ymaximum, steps_x, steps_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Broyden-Fletcher-Goldfarb-Shanno (BFGS)\n",
    "\n",
    "The BFGS method is one of the quasi-Newton methods and, as the name suggests, approximates the Newton-Raphson method discussed above.\n",
    "The major difference of a quasi-Newton method is that the Hessian is not calculated directly, but instead approximated.\n",
    "This approximation results in the BFGS method being more robust and often times converging more quickly than the Newton-Raphson method.\n",
    "   \n",
    " The only difference to the Newton method is the update of the approximate Hessian, $H_{approx}$ and the update of the search direction.\n",
    "The mathematics of these updates are out of the scope of these notebooks; however, the interested reader is directed towards *Polak, E. Computational Methods in Optimization; a Unified Approach. New York: Academic, 1971. Print.* for a more detailed explanation.\n",
    " \n",
    " The code below applies the BFGS method to the example function and plots the optimization path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# MAIN CODE\n",
    "##########\n",
    "old_guess = initial_guess\n",
    "\n",
    "approx_hessian = np.identity(2)\n",
    "inverse_approx_hessian = np.identity(2)\n",
    "\n",
    "num_steps = 0\n",
    "\n",
    "s_i = np.array([1.0, 0.0])\n",
    "y_i = np.array([0.0, 1.0])\n",
    "\n",
    "converged = False\n",
    "steps_x = []\n",
    "steps_y = []\n",
    "steps_x.append(old_guess[0])\n",
    "steps_y.append(old_guess[1])\n",
    "while not converged and num_steps <= IT_MAX:\n",
    "    print(\"STEP {:d}\".format(num_steps))\n",
    "    num_steps += 1\n",
    "    new_search_direction = np.dot(inverse_approx_hessian, -Gradient(old_guess))\n",
    "    try:\n",
    "        s_i = linesearch(f, Gradient, old_guess, new_search_direction)[\n",
    "            0] * new_search_direction\n",
    "    except:\n",
    "        print(\"WARNING: HESSIAN IS NOT POSITIVE DEFINITE... RESETTING SEARCH DIRECTION TO STEEPEST DESCENT!\")\n",
    "        new_search_direction = -Gradient(old_guess)\n",
    "        s_i = linesearch(f, Gradient, old_guess, new_search_direction)[\n",
    "            0] * new_search_direction\n",
    "\n",
    "    new_guess = old_guess + s_i\n",
    "    y_i = Gradient(new_guess) - Gradient(old_guess)\n",
    "    approx_hessian += ((np.outer(y_i, y_i)) / (np.dot(y_i, s_i))) - ((np.dot(approx_hessian,\n",
    "                                                                             np.dot(np.outer(s_i, s_i), approx_hessian))) / (np.dot(s_i, np.dot(approx_hessian, s_i))))\n",
    "    # update using Sherman-Morrison formula\n",
    "    part_A = np.dot(s_i, y_i)\n",
    "    part_A += np.dot(y_i, np.dot(inverse_approx_hessian, y_i))\n",
    "    part_A /= np.dot(s_i, y_i)**2\n",
    "    part_A *= np.outer(s_i, s_i)\n",
    "    part_B = np.dot(inverse_approx_hessian, np.outer(y_i, s_i))\n",
    "    part_B += np.dot(np.outer(s_i, y_i), inverse_approx_hessian)\n",
    "    part_B /= np.dot(s_i, y_i)\n",
    "    inverse_approx_hessian += part_A - part_B\n",
    "\n",
    "    diff = old_guess - new_guess\n",
    "    diff = np.sqrt(np.dot(diff, diff))\n",
    "    print (\"OLD GUESS: {}\".format(old_guess))\n",
    "    print (\"NEW GUESS: {}\".format(new_guess))\n",
    "    print (\"DIFFERENCE: {}\".format(diff))\n",
    "    old_guess = new_guess\n",
    "    old_search_direction = new_search_direction\n",
    "    steps_x.append(old_guess[0])\n",
    "    steps_y.append(old_guess[1])\n",
    "    if(diff < residual_tolerance):\n",
    "        converged = True\n",
    "steps_x = np.array(steps_x)\n",
    "steps_y = np.array(steps_y)\n",
    "steps_per_method[\"BFGS\"] = num_steps\n",
    "difference_per_method[\"BFGS\"] = diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pf.plot_func(xminimum, xmaximum, yminimum, ymaximum, steps_x, steps_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"{:->39}\".format(\"\"))\n",
    "print(\"{:>20} : {:>5} {:>10}\".format(\"Method\", \"Steps\", \"Difference\"))\n",
    "print(\"{:->20}   {:->5} {:->10}\".format(\"\", \"\", \"\"))\n",
    "for i in sorted(difference_per_method, key=difference_per_method.get, reverse=True):\n",
    "    print(\"{:>20} : {:>5} {:>6.4e}\".format(i, steps_per_method[i], difference_per_method[i]))\n",
    "print(\"{:->39}\".format(\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deciding on an optimization method to use, we consider (above) the number of steps required to converge.\n",
    "The Newton-Raphson method converges quickly, and with great accuracy.\n",
    "However the Newton-Raphson method requires the calculation of a Hessian, which is expensive. The BFGS method avoids the Hessian calculation and instead iteratively builds an approximate Hessian.\n",
    "The BFGS method also converges quickly and is widely used in real world applications.\n",
    "The conjugate gradient method requires more steps to converge, but is accurate.\n",
    "Finally, the steepest descent method, while slow, guarantees convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6 Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Optimization methods are commonly used in chemistry to find the lowest energy structure of a molecule, which corresponds to the equilibrium gas phase geometry.\n",
    "The basic approach is to use a computational method to evaluate the energy and gradients with respect to moves of the nuclei.\n",
    "This energy surface can be explored using optimization methods to locate the ideal geometry. \n",
    "\n",
    "The code box below uses the BFGS method to optimize the Hartree-Fock energy of a ring of 6 hydrogen atoms.\n",
    "The two parameters that will be optimized are the radius of the ring and the ratio between pairs of hydrogen atoms.\n",
    "Remember the BFGS method requires first derivatives, and in this case the first derivate is calculated numerically using the central difference method.\n",
    "\n",
    "There are two stable geometries; one with equally spaced hydrogens and another with \"dimers\" of hydrogens.\n",
    "\n",
    "Run the first cell to perform the optimization, and then run the second cell to visualize the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DIMERS   \n",
    "# radius = 4.5 # atomic units\n",
    "# deformation = 0.65 # ratio (must be less than or equal to 1)\n",
    "# RING\n",
    "radius = 2.0 # atomic units\n",
    "deformation = 1.0 # ratio (must be less than or equal to 1)\n",
    "\n",
    "if deformation > 1.0 or deformation < 0.0:\n",
    "        deformation = np.abs(deformation) % 1.0\n",
    "\n",
    "steps_radius, steps_deformation, steps_energy = pf.optimize_H6_ring(radius, deformation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from ipywidgets import *\n",
    "step_set = ipw.IntSlider(min=0,max =len(steps_energy)-1,value=0)\n",
    "interactive_plot = ipw.interactive(pf.plotting, step=step_set,steps_radius=fixed(steps_radius), steps_energy=fixed(steps_energy),steps_deformation=fixed(steps_deformation))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Please insert the number of the correct answer into the function below.\n",
    "Based on the energies of the two structures calculated above the ring with equally paired hydrogens is  \n",
    "    1. a global minimum\n",
    "    2. a local minimum\n",
    "    3. an equivilant structure to the paired hydrogens\n",
    "    4. a maximum\n",
    "    5. None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pf.question_one_check()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
