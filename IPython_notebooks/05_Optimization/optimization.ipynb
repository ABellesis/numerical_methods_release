{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization methods\n",
    "Optimization methods attempt to find the minimum or maximum of functions. Here the steepest descent, conjugate gradient, Newton-Raphson, and Broyden-Fletcher-Goldfarb-Shanno methods are presented.\n",
    "\n",
    "For all methods we will follow generally the same algorithm:\n",
    "```\n",
    "define and initial starting point:\n",
    "while converged = False & iteration < ITMAX\n",
    "    define the step direction\n",
    "    take a step in that direction\n",
    "    if convergence criteria met:\n",
    "        exit loop\n",
    "```\n",
    "Between all of the different methods presented in this notebook, they only change in defining the step direction. Alternatively, one thing that will stay consistent between all methods is the convergence criteria. An optimization algorithm should always test three convergence criteria\n",
    "1. update: to ensure we're not taking too big of a step.\n",
    "2. residual: to see if solution estimates are within errors of eachother\n",
    "3. iteration: set an iteration maximum so you do not get caught in an infinite loop if function is ilbehaved. Additionally this save you from crashing your computer if you're writing code yourself!\n",
    "\n",
    "# 4.1 Steepest Descent #\n",
    "\n",
    "The steepest descent method is the simplest iterative method of optimization. Steepest descent makes iterative steps in the direction of greatest decrease.\n",
    "\n",
    "First, an initial guess,$x_0$, is taken. The starting search direction, $sd_0$, is taken as the direction of steepest descent.\n",
    "\n",
    " $$ sd_0 = -\\nabla f(x_0)$$\n",
    "\n",
    "A step is taken by using a line search minimization along this search direction, \n",
    "\n",
    "$$ x_i = x_0 + \\lambda sd_0 $$\n",
    "\n",
    "where $\\lambda$ minimizes $f(x)$ in the search direction.\n",
    "\n",
    "A new search direction is constructed as the direction of steepest descent at the new point.\n",
    "\n",
    " $$ sd_i = -\\nabla f(x_i)$$\n",
    " \n",
    " \n",
    " The cell directly below implements steepest descent on the function,\n",
    " $$ f(x,y) = sin(0.5x^2-0.25y^2+3)*cos(2x+1-e^y)$$\n",
    "Following the code is an image depicting steepest descent implemented on this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "import numpy as np\n",
    "from scipy.optimize import line_search as linesearch\n",
    "import scipy.linalg as spla\n",
    "import scipy as sp\n",
    "import plotting_functions as pf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "\n",
    "def f(point):\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    return np.sin(0.5 * x**2 - 0.25 * y**2 + 3) * np.cos(2 * x + 1 - np.exp(y))\n",
    "\n",
    "\n",
    "def partial_f_partial_x(x, y):\n",
    "    part1 = x * np.cos(0.5 * x**2 - 0.25 * y**2 + 3) * \\\n",
    "        np.cos(2 * x + 1 - np.exp(y))\n",
    "    part2 = -2 * np.sin(0.5 * x**2 - 0.25 * y**2 + 3) * \\\n",
    "        np.sin(2 * x + 1 - np.exp(y))\n",
    "    return part1 + part2\n",
    "\n",
    "\n",
    "def partial_f_partial_y(x, y):\n",
    "    part1 = np.exp(y) * np.sin(0.5 * x**2 - 0.25 * y**2 + 3) * \\\n",
    "        np.sin(2 * x + 1 - np.exp(y))\n",
    "    part2 = -0.5 * y * \\\n",
    "        np.cos(0.5 * x**2 - 0.25 * y**2 + 3) * np.cos(2 * x + 1 - np.exp(y))\n",
    "    return part1 + part2\n",
    "\n",
    "\n",
    "def Gradient(point):\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    return np.array([partial_f_partial_x(x, y), partial_f_partial_y(x, y)])\n",
    "\n",
    "\n",
    "def partial_sq_f_partial_x_sq(x, y):\n",
    "    part1 = -4 * x * \\\n",
    "        np.cos(0.5 * x**2 - 0.25 * y**2 + 3) * np.sin(2 * x + 1 - np.exp(y))\n",
    "    part2 = -4 * np.sin(0.5 * x**2 - 0.25 * y**2 + 3) * \\\n",
    "        np.cos(2 * x + 1 - np.exp(y))\n",
    "    part3 = np.cos(2 * x + 1 - np.exp(y)) * (np.cos(0.5 * x**2 -\n",
    "                                                    0.25 * y**2 + 3) - (x**2) * np.sin(0.5 * x**2 - 0.25 * y**2 + 3))\n",
    "    return part1 + part2 + part3\n",
    "\n",
    "\n",
    "def partial_sq_f_partial_y_sq(x, y):\n",
    "    part1 = -np.exp(y) * y * np.cos(0.5 * x**2 - 0.25 * y **\n",
    "                                    2 + 3) * np.sin(2 * x + 1 - np.exp(y))\n",
    "    part2 = np.sin(0.5 * x**2 - 0.25 * y**2 + 3) * ((np.exp(y) * np.sin(2 *\n",
    "                                                                        x + 1 - np.exp(y))) - (np.exp(2 * y) * np.cos(2 * x + 1 - np.exp(y))))\n",
    "    part3 = np.cos(2 * x + 1 - np.exp(y)) * ((-0.25 * y**2 * np.sin(0.5 *\n",
    "                                                                    x**2 - 0.25 * y**2 + 3)) - (0.5 * np.cos(0.5 * x**2 - 0.25 * y**2 + 3)))\n",
    "    return part1 + part2 + part3\n",
    "\n",
    "\n",
    "def partial_sq_f_partial_x_y(x, y):\n",
    "    part1 = (x * np.exp(y)) * np.cos(0.5 * x**2 - 0.25 * y**2 + 3) * \\\n",
    "        np.sin(2 * x + 1 - np.exp(y))\n",
    "    part2 = y * np.cos(0.5 * x**2 - 0.25 * y**2 + 3) * \\\n",
    "        np.sin(2 * x + 1 - np.exp(y))\n",
    "    part3 = (2 * np.exp(y)) * np.sin(0.5 * x**2 - 0.25 * y**2 + 3) * \\\n",
    "        np.cos(2 * x + 1 - np.exp(y))\n",
    "    part4 = (0.5 * x * y) * np.sin(0.5 * x**2 - 0.25 * y**2 + 3) * \\\n",
    "        np.cos(2 * x + 1 - np.exp(y))\n",
    "    return part1 + part2 + part3 + part4\n",
    "\n",
    "\n",
    "def Hessian(point):\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    part1 = partial_sq_f_partial_x_sq(x, y)\n",
    "    part2 = partial_sq_f_partial_x_y(x, y)\n",
    "    part3 = partial_sq_f_partial_y_sq(x, y)\n",
    "    return np.array([[part1, part2], [part2, part3]])\n",
    "# If you don't have analytical gradients... Use numerical gradients! \n",
    "# def Central_Diff_X(a,y,h):\n",
    "#     return (f((a+h,y)) - f((a-h,y))) / (2*h)\n",
    "# def Central_Diff_Y(x,b,h):\n",
    "#     return (f((x,b+h)) - f((x,b-h))) / (2*h)\n",
    "# def Gradient(point):\n",
    "#     x= point[0]\n",
    "#     y = point[1]\n",
    "#     step = 1e-6\n",
    "#     return np.array([Central_Diff_X(x,y,step),Central_Diff_Y(x,y,step)])\n",
    "# def Central_Diff_2nd_X(a,y,h):\n",
    "# Approximation by Central Difference Method\n",
    "#         return (f((a+h,y)) - 2*f((a,y)) + f((a-h,y))) / (h**2)\n",
    "# def Central_Diff_2nd_Y(x,b,h):\n",
    "# Approximation by Central Difference Method\n",
    "#         return (f((x,b+h)) - 2*f((x,b)) + f((x,b-h))) / (h**2)\n",
    "# def Central_Diff_2nd_XY(a,b,h):\n",
    "# Approximation by Central Difference Method\n",
    "#         return (-1.0/(2*h**2))*(f((a+h,b))+f((a-h,b))+f((a,b+h))+f((a,b-h))-(2*f((a,b)))-f((a+h,b+h))-f((a-h,b-h)))\n",
    "# def Hessian(a,b):\n",
    "#     step = 1e-6\n",
    "#     return np.array([[Central_Diff_2nd_X(a,b,step),Central_Diff_2nd_XY(a,b,step)],[Central_Diff_2nd_XY(a,b,step),Central_Diff_2nd_Y(a,b,step)]])\n",
    "##########\n",
    "# GLOBAL VARS\n",
    "##########\n",
    "initial_guess = np.array((1.6, -1.5))  # initial guess\n",
    "#convergence criteria\n",
    "residual_tolerance = 1e-6  \n",
    "update_tolerance = 1e-5\n",
    "IT_MAX = 1000\n",
    "# domain for plotting\n",
    "xminimum = -1\n",
    "xmaximum = 2\n",
    "yminimum = -4\n",
    "ymaximum = 1\n",
    "steps_per_method = dict()\n",
    "difference_per_method = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########\n",
    "#MAIN CODE\n",
    "##########\n",
    "old_guess = initial_guess\n",
    "search_direction = -Gradient(old_guess) # set the search direction to the negative gradient at the initial guess\n",
    "converged = False \n",
    "num_steps = 0\n",
    "steps_x =[]\n",
    "steps_y =[]\n",
    "steps_x.append(old_guess[0])\n",
    "steps_y.append(old_guess[1])\n",
    "#repeat until convergenced or exceed IT_MAX\n",
    "while not converged and num_steps <= IT_MAX: \n",
    "    print(\"STEP {:d}\".format(num_steps))\n",
    "    num_steps+=1\n",
    "    gamma = linesearch(f,Gradient,old_guess,search_direction)[0] # find the step size that minimizes f along the search direction\n",
    "    if gamma == None: \n",
    "        print(\"ERROR: No appropriate step size found. Exiting minimization\")\n",
    "        exit()\n",
    "    else:\n",
    "        new_guess = old_guess + gamma*search_direction # update the current guess\n",
    "        search_direction = -Gradient(new_guess) # set the new search direction to the negative gradient at the new guess\n",
    "        diff = old_guess-new_guess # calculate the vector difference of the old guess and new guess\n",
    "        diff = np.sqrt(np.dot(diff,diff)) # calculate the magnitude of the separation between the old guess and the new guess\n",
    "        print(\"OLD GUESS: {}\".format(old_guess))\n",
    "        print(\"NEW GUESS: {}\".format(new_guess))\n",
    "        print(\"DIFFERENCE: {}\".format(diff))\n",
    "        old_guess = new_guess # update the old guess so we can take another step\n",
    "        steps_x.append(old_guess[0])\n",
    "        steps_y.append(old_guess[1])\n",
    "        if(diff<residual_tolerance): # if the step was small enough consider this the minimum\n",
    "            converged=True\n",
    "steps_x = np.array(steps_x)\n",
    "steps_y = np.array(steps_y)\n",
    "steps_per_method[\"Steepest Descent\"] = num_steps\n",
    "difference_per_method[\"Steepest Descent\"] = diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot_func(xminimum, xmaximum, yminimum, ymaximum, steps_x, steps_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Conjugate Gradient #\n",
    "\n",
    "Many similarities can be drawn with the steepest descent method, however the conjugate gradient attempts to ensure previous progress is not undone by the next steps. This restriction is called *conjugacy*. \n",
    "\n",
    "\n",
    "First an initial guess, $x_0$, is taken. The starting search direction, $sd_0$ is taken as the direction of steepest descent.\n",
    "\n",
    " $$ sd_0 = -\\nabla f(x_0)$$\n",
    "\n",
    "A step is taken by using a line search minimization along this search direction.\n",
    "\n",
    "A new search direction is constructed such that successive steps don't undo the progress of prior steps.\n",
    "\n",
    "This is done through a process similar to orthogonalization. \n",
    "\n",
    "$$ sd_{i+1} = -\\nabla f(x_{i+1}) + \\frac{(-\\nabla f(x_{i+1})) \\bullet (-\\nabla f(x_{i+1})) }{(-\\nabla f(x_{i})) \\bullet (-\\nabla f(x_{i}))} sd_{i} $$\n",
    "\n",
    "In practice, the line search minimization may fail because the search direction may point uphill.\n",
    "\n",
    "This can be resolved by reseting the search direction to point in the direction of steepest descent.\n",
    "\n",
    "The code below applies the conjugate gradient method to the example function and plots the optimization path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# MAIN CODE #\n",
    "#############\n",
    "converged = False\n",
    "num_steps = 0\n",
    "old_guess = initial_guess\n",
    "old_grad = -Gradient(old_guess)\n",
    "# set first search direction in the direction of steepest descent\n",
    "old_search_direction = old_grad\n",
    "steps_x = []\n",
    "steps_y = []\n",
    "steps_x.append(old_guess[0])\n",
    "steps_y.append(old_guess[1])\n",
    "while not converged and num_steps <= IT_MAX:\n",
    "    print(\"STEP {:d}\".format(num_steps))\n",
    "    alpha = linesearch(f, Gradient, old_guess, old_search_direction)[0]\n",
    "    if alpha == None:\n",
    "        print(\"WARNING: LINE SEARCH DID NOT CONVERGE... RESETTING SEARCH DIRECTION TO STEEPEST DESCENT!\")\n",
    "        old_search_direction = -Gradient(old_guess)\n",
    "        alpha = linesearch(f, Gradient, old_guess, old_search_direction)[0]  # step size\n",
    "    new_guess = old_guess + alpha * old_search_direction\n",
    "    new_grad = -Gradient(new_guess)\n",
    "    # gamma encompasses the fraction in the above equation\n",
    "    gamma = np.dot(new_grad - old_grad, new_grad) / np.dot(old_grad, old_grad)\n",
    "    # update the search direction\n",
    "    new_search_direction = new_grad + np.dot(gamma, old_search_direction)\n",
    "    diff = old_guess - new_guess\n",
    "    diff = np.sqrt(np.dot(diff, diff))\n",
    "    print(\"OLD GUESS: {}\".format(old_guess))\n",
    "    print(\"NEW GUESS: {}\".format(new_guess))\n",
    "    print(\"DIFFERENCE: {}\".format(diff))\n",
    "    num_steps += 1\n",
    "    old_guess = new_guess\n",
    "    old_grad = new_grad\n",
    "    old_search_direction = new_search_direction\n",
    "    steps_x.append(old_guess[0])\n",
    "    steps_y.append(old_guess[1])\n",
    "    if(diff < residual_tolerance):\n",
    "        converged = True\n",
    "steps_x = np.array(steps_x)\n",
    "steps_y = np.array(steps_y)\n",
    "steps_per_method[\"Conjugate Gradient\"] = num_steps\n",
    "difference_per_method[\"Conjugate Gradient\"] = diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.plot_func(xminimum, xmaximum, yminimum, ymaximum, steps_x, steps_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Newton-Raphson\n",
    "\n",
    "In the root-finding section of these lessons, the Newton-Raphson root finding method was described for a one-dimensional system. This method can be applied to higher dimensions by simple modifications.\n",
    "As a reminder, the Newton-Raphson method takes iterative steps:\n",
    "$$x_{i+1}\\ =\\ x_i - \\frac{f(x_i)}{f'(x_i)}$$\n",
    "\n",
    "To minimize the first derivative to find an extremum, the following iterative equation could be used:\n",
    "$$x_{i+1}\\ =\\ x_i - \\frac{f'(x_i)}{f''(x_i)}$$\n",
    "\n",
    "To optimize a higher order function, the Hessian matrix is introduced.\n",
    "The Hessian is a square matrix composed of second derivatives that describe the local curvature of a function. The iterative steps are given by:\n",
    "$$ p_i = (x_i,y_i) $$\n",
    "and\n",
    "$$p_{i+1} = p_i - \\gamma[H(p_i)]^{-1}]\\nabla f(p_i)$$\n",
    "\n",
    "where the step size, $\\gamma$, is found using a line search and $H(p_i)$ is the Hessian:\n",
    "$$H(p_i)= \\ \\begin{bmatrix}\n",
    "{\\frac{\\partial^2 f(x_i,y_i)}{\\partial x^2}} \n",
    "& {\\frac{\\partial^2 f(x_i,y_i)}{\\partial x \\partial y}} \\\\\n",
    "{\\frac{\\partial^2 f(x_i,y_i)}{\\partial y \\partial x}} \n",
    "& {\\frac{\\partial^2 f(x_i,y_i)}{\\partial y^2}} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "\n",
    "It is important to note that the Newton-Raphson method requires the Hessian to be positive definite. A positive definite Hessian ensures a local minimum near $p_i$ is isolated and not a saddle point or local maximum. We will see below that the Newton-Raphson method finds the minimum of a well-behaved, convex function, such as $x^2\\ +\\ y^2$. If we solve for the Hessian of this function:\n",
    "$$Hf(x,y)= \\ \\begin{bmatrix}\n",
    "{\\frac{\\partial^2 f(x,y)}{\\partial x^2}} \n",
    "& {\\frac{\\partial^2 f(x,y)}{\\partial x \\partial y}} \\\\\n",
    "{\\frac{\\partial^2 f(x,y)}{\\partial y \\partial x}} \n",
    "& {\\frac{\\partial^2 f(x,y)}{\\partial y^2}} \n",
    "\\end{bmatrix} $$\n",
    "$$Hf(x,y)= \\ \\begin{bmatrix}\n",
    "{\\frac{\\partial^2 (x^2+y^2)}{\\partial x^2}} \n",
    "& {\\frac{\\partial^2 (x^2+y^2)}{\\partial x \\partial y}} \\\\\n",
    "{\\frac{\\partial^2 (x^2+y^2)}{\\partial y \\partial x}} \n",
    "& {\\frac{\\partial^2 (x^2+y^2)}{\\partial y^2}} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$Hf(x,y)= \\ \\begin{bmatrix}\n",
    "2\n",
    "& 0 \\\\\n",
    "0 \n",
    "& 2 \n",
    "\\end{bmatrix} $$\n",
    "These postive values of the Hessian indicate that our function has a local minimum, thus Newton's method of optimization will perform well.\n",
    "Despite the requirement of a positive definite Hessian, the Newton-Raphson's method has excellent convergence since most functions near a minimum resemble $x^2\\ +\\ y^2$.\n",
    "\n",
    "The code below applies the Newton-Raphson optimization method to the example function and plots the optimization path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# MAIN CODE\n",
    "##########\n",
    "old_guess = initial_guess\n",
    "old_search_direction = -Gradient(old_guess)\n",
    "num_steps = 0\n",
    "converged = False\n",
    "steps_x = []\n",
    "steps_y = []\n",
    "steps_x.append(old_guess[0])\n",
    "steps_y.append(old_guess[1])\n",
    "while not converged and num_steps <= IT_MAX:\n",
    "    print(\"STEP {:d}\".format(num_steps))\n",
    "    num_steps += 1\n",
    "    A = Hessian(old_guess)\n",
    "    try:\n",
    "        if num_steps == 1:\n",
    "            new_search_direction = -Gradient(old_guess)\n",
    "            gamma = linesearch(\n",
    "                f, Gradient, old_guess, new_search_direction)[0]  # step size\n",
    "            new_guess = old_guess + gamma * new_search_direction\n",
    "        else:\n",
    "            new_search_direction = spla.solve(A, -Gradient(old_guess))\n",
    "            new_guess = old_guess + new_search_direction\n",
    "    except:\n",
    "        print(\"WARNING: HESSIAN IS NOT POSITIVE DEFINITE... RESETTING SEARCH DIRECTION TO STEEPEST DESCENT!\")\n",
    "        new_search_direction = -Gradient(old_guess)\n",
    "        gamma = linesearch(f, Gradient, old_guess, new_search_direction)[\n",
    "            0]  # step size\n",
    "        new_guess = old_guess + gamma * new_search_direction\n",
    "    diff = old_guess - new_guess\n",
    "    diff = np.sqrt(np.dot(diff, diff))\n",
    "    print(\"OLD GUESS: {}\".format(old_guess))\n",
    "    print(\"NEW GUESS: {}\".format(new_guess))\n",
    "    print(\"DIFFERENCE: {}\".format(diff))\n",
    "    old_guess = new_guess\n",
    "    steps_x.append(old_guess[0])\n",
    "    steps_y.append(old_guess[1])\n",
    "    old_search_direction = new_search_direction\n",
    "    if(diff < residual_tolerance):\n",
    "        converged = True\n",
    "steps_x = np.array(steps_x)\n",
    "steps_y = np.array(steps_y)\n",
    "steps_per_method[\"Newton-Raphson\"] = num_steps\n",
    "difference_per_method[\"Newton-Raphson\"] = diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot_func(xminimum, xmaximum, yminimum, ymaximum, steps_x, steps_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Broyden-Fletcher-Goldfarb-Shanno (BFGS)\n",
    "\n",
    "   The BFGS method is one of the quasi-Newton methods and, as the name suggests, is an approximation of the Newton optimization method discussed above. The major difference of these methods compared to the Newton method is the Hessian is not calculated directly, but instead approximated. This approximation results in the BFGS method being more robust and often times converging more quickly than the Newton method. \n",
    "   \n",
    " The only difference to the Newton method is the update of the approximate Hessian, $H_{approx}$ and the update of the search direction. The mathematics of these updates are out of the scope of these notebooks; however, the interested reader is directed towards *Polak, E. Computational Methods in Optimization; a Unified Approach. New York: Academic, 1971. Print.* for a more detailed explanation.\n",
    " \n",
    " The code below applies the BFGS method to the example function and plots the optimization path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# MAIN CODE\n",
    "##########\n",
    "old_guess = initial_guess\n",
    "\n",
    "approx_hessian = np.identity(2)\n",
    "inverse_approx_hessian = np.identity(2)\n",
    "\n",
    "num_steps = 0\n",
    "\n",
    "s_i = np.array([1.0, 0.0])\n",
    "y_i = np.array([0.0, 1.0])\n",
    "\n",
    "converged = False\n",
    "steps_x = []\n",
    "steps_y = []\n",
    "steps_x.append(old_guess[0])\n",
    "steps_y.append(old_guess[1])\n",
    "while not converged and num_steps <= IT_MAX:\n",
    "    print(\"STEP {:d}\".format(num_steps))\n",
    "    num_steps += 1\n",
    "    new_search_direction = np.dot(inverse_approx_hessian, -Gradient(old_guess))\n",
    "    try:\n",
    "        s_i = linesearch(f, Gradient, old_guess, new_search_direction)[\n",
    "            0] * new_search_direction\n",
    "    except:\n",
    "        print(\"WARNING: HESSIAN IS NOT POSITIVE DEFINITE... RESETTING SEARCH DIRECTION TO STEEPEST DESCENT!\")\n",
    "        new_search_direction = -Gradient(old_guess)\n",
    "        s_i = linesearch(f, Gradient, old_guess, new_search_direction)[\n",
    "            0] * new_search_direction\n",
    "\n",
    "    new_guess = old_guess + s_i\n",
    "    y_i = Gradient(new_guess) - Gradient(old_guess)\n",
    "    approx_hessian += ((np.outer(y_i, y_i)) / (np.dot(y_i, s_i))) - ((np.dot(approx_hessian,\n",
    "                                                                             np.dot(np.outer(s_i, s_i), approx_hessian))) / (np.dot(s_i, np.dot(approx_hessian, s_i))))\n",
    "    # update using Sherman-Morrison formula\n",
    "    part_A = np.dot(s_i, y_i)\n",
    "    part_A += np.dot(y_i, np.dot(inverse_approx_hessian, y_i))\n",
    "    part_A /= np.dot(s_i, y_i)**2\n",
    "    part_A *= np.outer(s_i, s_i)\n",
    "    part_B = np.dot(inverse_approx_hessian, np.outer(y_i, s_i))\n",
    "    part_B += np.dot(np.outer(s_i, y_i), inverse_approx_hessian)\n",
    "    part_B /= np.dot(s_i, y_i)\n",
    "    inverse_approx_hessian += part_A - part_B\n",
    "\n",
    "    diff = old_guess - new_guess\n",
    "    diff = np.sqrt(np.dot(diff, diff))\n",
    "    print (\"OLD GUESS: {}\".format(old_guess))\n",
    "    print (\"NEW GUESS: {}\".format(new_guess))\n",
    "    print (\"DIFFERENCE: {}\".format(diff))\n",
    "    old_guess = new_guess\n",
    "    old_search_direction = new_search_direction\n",
    "    steps_x.append(old_guess[0])\n",
    "    steps_y.append(old_guess[1])\n",
    "    if(diff < residual_tolerance):\n",
    "        converged = True\n",
    "steps_x = np.array(steps_x)\n",
    "steps_y = np.array(steps_y)\n",
    "steps_per_method[\"BFGS\"] = num_steps\n",
    "difference_per_method[\"BFGS\"] = diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot_func(xminimum, xmaximum, yminimum, ymaximum, steps_x, steps_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 What method to choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:->39}\".format(\"\"))\n",
    "print(\"{:>20} : {:>5} {:>10}\".format(\"Method\", \"Steps\", \"Difference\"))\n",
    "print(\"{:->20}   {:->5} {:->10}\".format(\"\", \"\", \"\"))\n",
    "for i in sorted(difference_per_method, key=difference_per_method.get, reverse=True):\n",
    "    print(\"{:>20} : {:>5} {:>6.4e}\".format(i, steps_per_method[i], difference_per_method[i]))\n",
    "print(\"{:->39}\".format(\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMERS   \n",
    "# radius = 4.5 # atomic units\n",
    "# deformation = 0.65 # ratio (must be less than or equal to 1)\n",
    "# RING\n",
    "radius = 2.0 # atomic units\n",
    "deformation = 1.0 # ratio (must be less than or equal to 1)\n",
    "\n",
    "\n",
    "\n",
    "if deformation > 1.0 or deformation < 0.0:\n",
    "        deformation = np.abs(deformation) % 1.0\n",
    "\n",
    "steps_radius, steps_deformation, steps_energy = pf.optimize_H6_ring(radius, deformation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from ipywidgets import *\n",
    "step_set = ipw.IntSlider(min=0,max =len(steps_energy)-1,value=0)\n",
    "interactive_plot = ipw.interactive(pf.plotting, step=step_set,steps_radius=fixed(steps_radius), steps_energy=fixed(steps_energy),steps_deformation=fixed(steps_deformation))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deciding on an optimization method to use, we consider (above) the number of steps required to converge. The Newton-Raphson method converges quickly, and with great accuracy. However the Newton-Raphson method requires the calculation of a Hessian, which is expensive. The BFGS method avoids the Hessian calculation and instead iteratively builds an approximate Hessian. The BFGS method also converges quickly and is widely used in real world applications. The congujate gradient method requires more steps to converge, but is accurate. Finally, the steepest descent method, while slow, guarantees convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Your Task\n",
    "\n",
    "Optimization methods are commonly used in chemistry to find the lowest energy structure of a molecule, which corresponds to the equilibrium gas phase geometry. The basic approach is to use a computational method to evaluate the energy and gradients with respect to moves of the nuclei, and using this information with an optimization method to optimize the geometry. \n",
    "\n",
    "Given what you have learned about optimization methods:\n",
    "What method(s) would you use to optimize the structure of a molecule?\n",
    "Consider the expense/availability of 2nd derivatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
