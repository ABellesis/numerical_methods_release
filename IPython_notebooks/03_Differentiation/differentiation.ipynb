{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** 2. Differentiation**\n",
    "\n",
    "Calculating an analytical solution to a derivative is not always feasible or possible. In such a situation, we turn to numerical differentiation. This notebook discusses two numerical differentiation methods, the forward difference and central difference methods. \n",
    "\n",
    "In order to derive the forward and central difference formulas, we begin with a Taylor series:\n",
    "\n",
    "$$ f(x)\\ =\\ f(a)\\ +\\ \\frac{f'(a)}{1!}(x-a)\\ +\\ \\frac{f''(a)}{2!}(x-a)^2\\ +\\ \\frac{f'''(a)}{3!}(x-a)^3\\ +\\ \\dots\\ +\\ \\frac{f^{(n)}(a)}{n!}(x-a)^n\\ + \\dots$$\n",
    "\n",
    "where $a$ is the point about which the series is expanded. Performing a substitution using $(x\\ -\\ a) = h$:\n",
    "\n",
    "$$ f(x)\\ =\\ f(a)\\ +\\ \\frac{f'(a)}{1!}h\\ +\\ \\frac{f''(a)}{2!}h^2\\ +\\ \\frac{f'''(a)}{3!}h^3\\ +\\ \\dots\\ +\\ \\frac{f^{(n)}(a)}{n!}h^n\\ + \\dots$$\n",
    "\n",
    "Here, $h$ is the step size for the differentiation. The choice of $h$ must be taken with great care as it has the potential of introducing error rendering the solution incorrect and meaningless. The effects of this can be seen by considering the simple form of a derivative:\n",
    "\n",
    " $$f'(x)\\ =\\ \\frac{f(x+h)-f(x)}{h}$$\n",
    " \n",
    "A large value of $h$ will cause unrealistic results or a divergence as large steps neglect to capture the behavior of the function's derivative. On the other hand, a small value of $h$ introduces round-off errors because a computer can only store numbers to a certain precision (often ~ $10^{-15}$ for personal computers). A step size smaller than this precision will return incorrect results. Round-off error can be introduced into numerical methods in many ways:\n",
    "\n",
    "1. Addition of nearly equal numbers\n",
    "2. Subtracting a small number from a large number\n",
    "3. Division of by a small number\n",
    "3. User input error (pi = 3.1415)\n",
    "4. Data type coding error\n",
    "\n",
    "In differentiation, round-off errors results from the addition of nearly equal number. Thus an optimal value of $h$ exists which offers a trade off between the two forms of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ** 2.1  Forward Difference Formula for First Derivatives **\n",
    "\n",
    "In order to obtain the forward difference formula, the Taylor series is truncated after the first derivative term. \n",
    "\n",
    "This gives the following\n",
    "\n",
    "$$ f(x)\\ \\approx\\ f(a+h)\\ =\\ f(a)\\ +\\ hf'(a)\\ +\\ \\mathcal{O}(h^2)$$\n",
    "\n",
    "The $O(h^2)$ term might be new to some of you. This notation indicates the magnitude of the error introduced by truncating the Taylor series, For this reason it is referred to as the truncation error. Rearranging the above expression to isolate $f'(a)$, as shown below, represents the forward difference approximation:\n",
    "\n",
    "$$ f'(a)\\ =\\ \\frac{f(a+h)\\ -\\ f(a)}{h} + \\mathcal{O}(h^2)$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below imports the necessary modules, defines the function we will evaluate, the corresponding analytical derivative, and the initial variables.\n",
    "\n",
    "\n",
    "**An important note for this notebooks:** For this notebook, use the Sympy function to define additional functions. This means when you want to call to a function from the Sympy library, import them as `syp.sin()`, `syp.cos()`, `syp.exp(`). The Sympy package will help us calculate the analytical gradients using Python, enabling you to check errors of differentiation from the true result, but saving you the trouble from calculating them by hand.  \n",
    "\n",
    "## Your task\n",
    "1. The general structure of the function `f` is given. You need to code the actual function. To the right of `result =`, define the function $x^2$ remember that exponentiation is accomplished in Python with **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "# These are packages needed to perform the math and visualization\n",
    "import numpy as np\n",
    "import IPython\n",
    "from IPython.display import Math\n",
    "import matplotlib.pyplot as plt\n",
    "import plotting_functions as pf\n",
    "import sympy as syp\n",
    "syp.init_printing(use_unicode=False, wrap_line=False, no_global=True)\n",
    "%matplotlib notebook\n",
    "\n",
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Defining the function to be evaluated\n",
    "    INPUT\n",
    "        float x\n",
    "    OUTPUT\n",
    "        float \n",
    "    \"\"\"\n",
    "    result = x**2\n",
    "    return result\n",
    "\n",
    "\n",
    "###############\n",
    "# GLOBAL VARS #\n",
    "###############\n",
    "# point at which we would like to evaluate the derivative\n",
    "x = 1.0\n",
    "# step size: distance between x and a, dx\n",
    "h = 1\n",
    "# the range of x values we would like to plot between:\n",
    "x_minimum = -1.5\n",
    "x_maximum = 2\n",
    "#exact derivative to compare our approximation to.\n",
    "exact_first_derivative = pf.analytical_first_derivative(f,x)\n",
    "exact_second_derivative = pf.analytical_second_derivative(f,x)\n",
    "# dictionaries to hold error of each method\n",
    "error_1 = dict()\n",
    "error_2 = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task!\n",
    "1. Answer the following multiple choice question. To answer place the number of the correct choice in the Python function below.\n",
    "\n",
    "### Question 1\n",
    "This type of error will dominate when the step size is small:\n",
    "      1. discretization error\n",
    "      2. truncation error\n",
    "      3. round-off error\n",
    "      4. speculation error\n",
    "      5. none of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.question_one_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code implementing the forward difference approximation of $f(x)$ is shown below.  The Python code considers the value approximatedby the method and calculates the exact value to report the error.\n",
    "\n",
    "In the plot below, a geometric interpretation of the forward difference approximation is given to the function you defined above.  Here, the tangent line through $f(a)$ is shown in blue. The approximation is shown between the $f(a)$ (blue point) and $f(a+h)$ (red point). The approximation to the first derivative is taken as the secant line between the two points, shown as the red line. A graphical representation of the forward difference approximation (in red) implemented on $f(x)$ (in black) is shown below. What you should notice is that the approximation, is not quite overlaid with the actual tangent line. You will also see a warning printed that our step size is too large. \n",
    "\n",
    "## Your Task\n",
    "1. Set the step size to the value of h = 0.5  in the cell below. Has the approximation improved?\n",
    "2. Set the step size to the value of h = 1e-5  in the cell below. Has the approximation improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "def Forward_Diff(f, a, h):\n",
    "    \"\"\"\n",
    "    Calculates the first derivative based on the Forward Difference method\n",
    "    INPUT\n",
    "        float a: the initial starting point\n",
    "        float h: the step size\n",
    "    OUTPUT\n",
    "        float: first derivative evaluated at a\n",
    "    \"\"\"\n",
    "    fd_step_size_result = pf.fd_step_size_check(f,h,a)\n",
    "    print(fd_step_size_result['message'])\n",
    "    return (f(a + h) - f(a)) / h\n",
    "\n",
    "\n",
    "#############\n",
    "# MAIN CODE #\n",
    "#############\n",
    "h = 0.5\n",
    "approximation = Forward_Diff(f, x, h)\n",
    "method = \"Forward Difference\"\n",
    "error_1[method] = approximation - exact_first_derivative\n",
    "print(\"Method: {}\".format(method))\n",
    "print(\"Approximation of f'({}) = {}\".format(x, approximation))\n",
    "print(\"Exact Evaluation of f'({}) = {}\".format(x,exact_first_derivative))\n",
    "print(\"Error = {}\".format(error_1[\"Forward Difference\"]))\n",
    "#input variables are:\n",
    "#(x minimum, x maximum, function, derivative of f, x, step size)\n",
    "\n",
    "pf.plot_fdiff_func(x_minimum, x_maximum, f, pf.analytical_first_derivative, x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** 2.2 Forward Difference Determination of Second Derivatives **\n",
    "\n",
    "The forward difference method can be extended to second derivatives. This requires two points that are spaced by an interval of $h$, $x = a+h$ and $x= a+2h$.\n",
    "Each of these terms are expanded into their respective Taylor series:\n",
    "\n",
    "$x=a+h$\n",
    "$$f(a+h) = f(a)+ f'(a)(h) + \\frac{f''(a)(h)^2}{2} + \\mathcal{O}(h^3)$$\n",
    "\n",
    "$$f'(a)= \\frac{f(a+h) - f(a) - \\frac{f''(a)(h)^2}{2}}{h} + \\mathcal{O}(h^3)$$\n",
    "\n",
    "$x=a+2h$\n",
    "$$ f(a+2h) = f(a)+ f'(a)(2h) + \\frac{f''(a)(2h)^2}{2} + \\mathcal{O}(h^3)$$\n",
    "\n",
    "  \n",
    "\n",
    "$$ f'(a)= \\frac{f(a+2h)- f(a) -f''(a)(2h^2)}{2h} + \\mathcal{O}(h^3)$$\n",
    "\n",
    "We can set these two terms equal to each other:\n",
    "\n",
    "$$\\frac{f(a+h) - f(a) - \\frac{f''(a)(h)^2}{2}}{h} + \\mathcal{O}(h^3)=\\frac{f(a+2h)- f(a) -f''(a)(2h^2)}{2h} + \\mathcal{O}(h^3)$$\n",
    "multiply by $2h$ on both sides,\n",
    "\n",
    "$$2f(a+h) - 2f(a) - f''(a)(h)^2=f(a+2h)- f(a) -f''(a)(2h^2) + \\mathcal{O}(h^3)$$\n",
    "\n",
    "collect the like terms, isolating $f''(a)$\n",
    "\n",
    "$$f''(a)(2h^2) - f''(a)(h)^2= f(a+2h) -2f(a+h)+ 2f(a) -f(a)  + \\mathcal{O}(h^3)$$\n",
    "\n",
    "Simplifying this expression results in:\n",
    "$$ f''(a)\\ =\\ \\frac{f(a+2h)-2f(a+h)+f(a)}{h^2} + \\mathcal{O}(h^3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task \n",
    "1. Answer the following multiple choice question. To answer place the number of the correct choice in the Python function below.\n",
    "\n",
    "### Question 2\n",
    "The $\\mathcal{O}(h^3)$ term at the end of the forward difference approximation __\n",
    "      1. represents that this approximation requires three points, one in the center and one to each side.\n",
    "      2. represents the round-off error introduced.\n",
    "      3. represents exactly the error of all remaining terms in the infinite Taylor series.\n",
    "      4. approximately represents the discretization error introduced by truncating the Taylor series.\n",
    "      5. none of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.question_two_check(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "import numpy as np\n",
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "\n",
    "def Forward_Diff_2nd(f, a, h):\n",
    "    \"\"\"\n",
    "    Calculates the second derivative based on the forward difference method\n",
    "    INPUT\n",
    "        float a: the initial starting point\n",
    "        float h: the step size\n",
    "    OUTPUT\n",
    "        float: second derivative evaluated at a\n",
    "    \"\"\"\n",
    "    return (f(a + 2 * h) - 2 * f(a + h) + f(a)) / (h**2)\n",
    "#############\n",
    "# MAIN CODE #\n",
    "#############\n",
    "approximate = Forward_Diff_2nd(f, x, h)\n",
    "method = \"Forward Difference\"\n",
    "error_2[method] = approximate - exact_second_derivative\n",
    "print(\"Method: {}\".format(method))\n",
    "print(\"Approximation of f''({}) = {}\".format(x, approximate))\n",
    "print(\"Exact Evaluation of f''({}) = {}\".format(x, exact_second_derivative))\n",
    "print(\"Error = {}\".format(error_2[\"Forward Difference\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** 2.3  Central Difference Formula for First Derivatives**\n",
    "\n",
    "The forward difference formula is one estimation of the derivative, but introduces large truncation error and thus we want to consider more accurate methods. The central difference formula can also provide useful approximation for first derivatives with a reduction of truncation error.  In order to obtain this relationship, one takes two similar Taylor expressions as shown above for the forward difference formula. the difference being that the negative direction is considered along with the forward. This will capture behavior of the curve surrounding the expansion point. \n",
    "\n",
    "$$ f(a+h)\\ =\\ f(a)\\ +\\ hf'(a)\\ +\\ \\frac{h^2}{2}f''(a)\\ +\\ \\ \\mathcal{O}(h^3) $$\n",
    "\n",
    "$$ f(a-h)\\ =\\ f(a)\\ -\\ hf'(a)\\ +\\ \\frac{h^2}{2}f''(a)\\ +\\ \\ \\mathcal{O}(h^3) $$\n",
    "\n",
    "Subtracting the second Taylor series expression from the first gives:\n",
    "\n",
    "$$ f(a+h)\\ -\\ f(a-h)\\ =\\ 2hf'(a)  + \\mathcal{O}(h^3)$$\n",
    "\n",
    "We rearrange and isolate for $f'(a)$:\n",
    "\n",
    "$$ f'(a)\\ =\\ \\frac{f(a+h)\\ -\\ f(a-h)}{2h} +\\mathcal{O}(h^3)$$\n",
    " \n",
    "\n",
    "The code implementing the central difference approximation on $f(x)$ is shown below. The Python code determines the true value to calculate the error associated with the central difference approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "\n",
    "def Central_Diff(f, a, h):\n",
    "    \"\"\"\n",
    "    Calculates the second derivative based on the central difference method\n",
    "    INPUT\n",
    "        float a: the initial starting point\n",
    "        float h: the step size\n",
    "    OUTPUT\n",
    "        float: first derivative evaluated at a\n",
    "    \"\"\"\n",
    "    cd_step_size_result = pf.cd_step_size_check(f,h,x)\n",
    "    print(cd_step_size_result['message'])\n",
    "    return (f(a + h) - f(a - h)) / (2 * h)\n",
    "\n",
    "#############\n",
    "# MAIN CODE #\n",
    "#############\n",
    "approximation = Central_Diff(f, x, h)\n",
    "method = \"Central Difference\"\n",
    "error_1[method] = approximation - exact_first_derivative\n",
    "print(\"Method: {}\".format(method))\n",
    "print(\"Approximation of f'({}) = {}\".format(x, approximation))\n",
    "print(\"Exact Evaluation of f'({})  = {}\".format(x,exact_first_derivative))\n",
    "print(\"Error = {}\".format(error_1[\"Central Difference\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graphical representation of the central difference approximation of the first dervative shows after the plotting command below is ran. In the plot, $a$ is the red point with the tangent line at $(a,\\ f(a))$ shown in blue representing the analytical value of the derivative at that point. The central difference approximation occurs at $(a+h,\\ f(a+h))$ and $(a-h,\\ f(a-h))$, the red points. The red line is the secant line. By comparison of the two methods, we see that the central differnce method gives a much better approximation of the first derivative as compared to the forward difference method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input variables are:\n",
    "#(x minimum, x maximum, function, derivative of f, x, step size)\n",
    "pf.plot_cdiff_func(x_minimum, x_maximum, f, pf.analytical_first_derivative, x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** 2.4  Central Difference Formula for Second Derivatives **\n",
    "\n",
    "The previously discussed central difference formula can also be utilized to approximate the second derivative of a function.  Taking our two Taylor series expressions from above:\n",
    "\n",
    "$$ f(a+h)\\ =\\ f(a)\\ +\\ hf'(a)\\ +\\ \\frac{h^2}{2}f''(a)\\ +\\ \\frac{h^3}{6}f'''(a)\\ +\\ \\mathcal{O}(h^4) $$\n",
    "\n",
    "$$ f(a-h)\\ =\\ f(a)\\ -\\ hf'(a)\\ +\\ \\frac{h^2}{2}f''(a)\\ -\\ \\frac{h^3}{6}f'''(a)\\ +\\ \\mathcal{O}(h^4) $$\n",
    "\n",
    "we add these expressions in this case as opposed to the subtraction that was used to obtain the central difference formula for first derivatives.  Upon addition, we obtain:\n",
    "\n",
    "$$ f(a+h)\\ +\\ f(a-h)\\ =\\ 2f(a)\\ +\\ h^2f''(a)\\ +\\ \\mathcal{O}(h^4) $$\n",
    "\n",
    "Rearrange and isolate for $f''(a)$ to obtain:\n",
    "\n",
    "$$ f''(a)\\ =\\ \\frac{f(a+h)\\ -\\ 2f(a)\\ +\\ f(a-h)}{h^2} + \\mathcal{O}(h^4) $$\n",
    "\n",
    "The following Python code illustrates the central difference formula applied to the same function discussed previously. The Python code determines the true value to calculate the error associated with the central difference approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "\n",
    "def Central_Diff_2nd(f, a, h):\n",
    "    \"\"\"\n",
    "    Calculates the second derivative based on the Central Difference method\n",
    "    INPUT\n",
    "        float a: the initial starting point\n",
    "        float h: the step size\n",
    "    OUTPUT\n",
    "        float: second derivative evaluated at a\n",
    "    \"\"\"\n",
    "    return (f(a + h) - 2 * f(a) + f(a - h)) / (h**2)\n",
    "#############\n",
    "# MAIN CODE #\n",
    "#############\n",
    "approximation = Central_Diff_2nd(f, x, h)\n",
    "method = \"Central Difference\"\n",
    "error_2[method] = approximation - exact_second_derivative\n",
    "print(\"Method: {}\".format(method))\n",
    "print(\"Approximation of f''({}) = {}\".format(x, approximation))\n",
    "print(\"Exact Evaluation of f''({}) = {}\".format(x,exact_second_derivative))\n",
    "print(\"Error = {}\".format(error_2[\"Central Difference\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2.5 Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:-^39}\".format(\"\"))\n",
    "print(\"{:^39}\".format(\"FIRST DERIVATIVE\"))\n",
    "print(\"{:-^39}\".format(\"\"))\n",
    "print(\"{:^25} : {:^11}\".format(\"Method\", \"Error\"))\n",
    "print(\"{:->25}   {:->11}\".format(\"\", \"\"))\n",
    "for i in sorted(error_1, key=error_1.get, reverse=True):\n",
    "    print( \"{:>25} : {:>.5E}\".format(i, error_1[i]))\n",
    "print( \"{:-^39}\".format(\"\"))\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"{:-^39}\".format(\"\"))\n",
    "print(\"{:^39}\".format(\"SECOND DERIVATIVE\"))\n",
    "print(\"{:-^39}\".format(\"\"))\n",
    "print(\"{:^25} : {:^11}\".format(\"Method\", \"Error\"))\n",
    "print(\"{:->25}   {:->11}\".format(\"\", \"\"))\n",
    "for i in sorted(error_2, key=error_2.get, reverse=True):\n",
    "    print(\"{:>25} : {:>.5E}\".format(i, error_2[i]))\n",
    "print(\"{:-^39}\".format(\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One should observe the significant improvement that central difference provides over forward difference. This improvement comes with a reduction in discretization error. Recall that the error is on the order of $ \\mathcal{O}(h^2) $ for forward difference, while the  order is $\\mathcal{O}(h^3)$ for central difference. Thus for a given step size, central difference will have a lower error and is generally be a better choice for discretization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Black Body Radiation\n",
    "\n",
    "Black body radiation is the electromagnetic radiation (light) given off by an opaque object that is in thermal equilibrium with its surroundings.\n",
    "Describing black body radiation was a monumental problem at the turn of the 20th century.\n",
    "Classical physics predicted that distribution of emitted light would approach infinity as the wavelength decreased (see the dotted line in the first plot before).\n",
    "This distribution is the Rayleigh-Jeans law:\n",
    "\n",
    "$$ B(\\lambda, T ) = \\frac{2 c k_B T}{\\lambda^4} $$\n",
    "\n",
    "where B is spectral radiance, $\\lambda$ is the wavelength of the light, $c$ is the speed of light, $k_B$ is Boltzmann's constant, and $T$ is the temperature.\n",
    "\n",
    "However, this behavior would contradict the principle of conservation of energy.\n",
    "This is known as the ultraviolet catastrophe.\n",
    "Quantum mechanics properly describes black bod radiation.\n",
    "\n",
    "The proper distribution of spectral radiance is given by\n",
    "\n",
    "$$B(\\lambda, T ) = \\frac{2hc^2}{\\lambda^5} \\frac{1}{e^{\\frac{hc}{\\lambda k_B T}}-1}$$\n",
    "\n",
    "where $h$ is Planck's constant, $\\lambda$ is the wavelength of the light, $c$ is the speed of light, $k_B$ is Boltzmann's constant, and $T$ is the temperature.\n",
    " \n",
    "## Your Task\n",
    "1. Run the code block below to observe the distribution of wavelengths corresponding to the Rayleigh-Jeans law (yellow), and the true distribution (blue line). Notice the erroneous intensity predicted by the Rayleigh-Jeans classically as the wavelength decreases. The true distribution ( blue line) has a maximum intensity that can be found where the first derivative is zero. This interactive plot finds the derivative at points specified by the user. Move the sliding bar for the wavelength to see the derivative numerically and graphically (dotted line). You will use this interactive graph to complete the exercise below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.html.widgets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button\n",
    "import scipy.constants as sc\n",
    "\n",
    "def Central_Diff(f, a, h,temperature):\n",
    "    \"\"\"\n",
    "    Calculates the second derivative based on the Central Difference method\n",
    "    INPUT\n",
    "        float a: the initial starting point\n",
    "        float h: the step size\n",
    "    OUTPUT\n",
    "        float: first derivative evaluated at a\n",
    "    \"\"\"\n",
    "    a *= 1e-9 # m to nm\n",
    "    h *= 1e-9 # m to nm\n",
    "    step_size_result=cd_step_size_result = pf.cd_step_size_check(f,h,a)\n",
    "    if step_size_result['result'] == True:\n",
    "        print(step_size_result['message'])\n",
    "        return (f(a + h,temperature) - f(a - h,temperature)) / (2 * h)\n",
    "    else:\n",
    "        print(step_size_result['message'])\n",
    "    return (f(a + h,temperature) - f(a - h,temperature)) / (2 * h)\n",
    "\n",
    "\n",
    "# Black body radiation\n",
    "def black_body_radiation(wavelength,T):\n",
    "    \"\"\"\n",
    "    Calculates the spectral radiance based on black-body radiation formula\n",
    "    INPUT:\n",
    "        wavelength (float): the wavelength in nm\n",
    "        T (float): in Kelvin\n",
    "    OUTPUT:\n",
    "     spectral_raidance (float): spectral radience in units of () \n",
    "    \"\"\"\n",
    "    h = sc.h # Planck's law units = J s\n",
    "    c = sc.c # units = m s^-1\n",
    "    k = sc.k # Boltzmann's constant units J K^-1\n",
    "    numerator = 2.0 * h * (c**2)\n",
    "    exponent = (h*c)/(wavelength*k*T)\n",
    "    denominator = (wavelength**5) * (np.exp(exponent) - 1.0)\n",
    "    spectral_radiance = numerator / denominator\n",
    "    spectral_radiance *= 1e-9 # m to nm\n",
    "    spectral_radiance *= 1e-3 # W to kW\n",
    "    return spectral_radiance # units kw/nm\n",
    "\n",
    "def RayleighJeans(wavelength,T):\n",
    "    \"\"\"\n",
    "    Calculates the spectral radiance based ton the Rayleigh-Jeans formula\n",
    "    INPUT:\n",
    "        wavelength (float): the wavelength in m\n",
    "        T (float): in Kelvin\n",
    "    \"\"\"\n",
    "    h = sc.h # units = Js\n",
    "    c = sc.c #units = ms^-1\n",
    "    k = sc.k # units J K^-1\n",
    "    numerator = 2*c*k*T\n",
    "    denominator = wavelength**4\n",
    "    spectral_radiance = numerator/denominator\n",
    "    spectral_radiance *= 1e-9 # m to nm\n",
    "    spectral_radiance *= 1e-3 # W to kW\n",
    "    return spectral_radiance\n",
    "    \n",
    "\n",
    "# Set up variables\n",
    "def plyt(T,lmbda):\n",
    "    wavelength = np.linspace(1e-8, 3e-6, 100)\n",
    "    # calculate spectral radiance\n",
    "    radiance = black_body_radiation(wavelength, T)\n",
    "    rj_radiance = RayleighJeans(wavelength, T)\n",
    "    #calculate the central difference derivative\n",
    "    step_size = 1e-6\n",
    "    derivative = Central_Diff(black_body_radiation, lmbda, step_size, T)\n",
    "    #plotting\n",
    "    fig,ax = plt.subplots(figsize=(8,4))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('wavelength (wavelength)')\n",
    "    plt.ylabel('spectral radiance')\n",
    "    line, = ax.plot(wavelength*1e9, radiance, label = 'black-body')\n",
    "    line_rj, = ax.plot(wavelength*1e9, rj_radiance, label = 'Rayleigh-Jeans')\n",
    "    line2, = ax.plot([lmbda,lmbda], [0,black_body_radiation(lmbda*1e-9,T)], color='red')\n",
    "    plot_derivative=black_body_radiation(lmbda*(1e-9), T) + derivative*(wavelength - lmbda*(1e-9))\n",
    "    line3, = ax.plot(wavelength*1e9,  plot_derivative, color='black', linestyle=\"--\", label = 'derivative')\n",
    "    plt.ylim(0.7*min(radiance), 1.3*max(radiance))\n",
    "    y_ax = ax.set_ylim()\n",
    "    x_ax = ax.set_xlim()\n",
    "    lambda_max_annotation = ax.annotate(r'$\\lambda$ = {:.2f}'.format(lmbda), xy = (0.8*x_ax[1],0.9*y_ax[1]))\n",
    "    deriv_annotation = ax.annotate(r\"$f'(\\lambda)$ = {:.2e}\".format(derivative*1e-9), xy = (0.75*x_ax[1],0.8*y_ax[1]))\n",
    "    ########\n",
    "    plt.legend(loc=5)\n",
    "    plt.show()\n",
    "\n",
    "interact(plyt,T =(3000,5000),lmbda=(1,2000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wien's Displacement Law\n",
    "Wien found that the distribution curve is inversely proportional to temperature.\n",
    "From this relationship, an expression for the maximum wavelength with the maximum intensity, $\\lambda_{max}$ is given by:\n",
    "\n",
    " $$ \\lambda_{max} = \\frac{b}{T}$$\n",
    "\n",
    " where $b$ is a proportionality constant and $T$ is the absolute temperature.\n",
    " The proportionality constant must be determined numerically. The true value is $b= 2.897729 \\times 10^6$nm K.\n",
    "\n",
    "# Exercise\n",
    "\n",
    "To highlight the practicality of numerical differentiation, we will Wien's proportionality constant using the central difference method.\n",
    "The constant can be found by plotting $\\lambda$ distributions for a series of temperature. \n",
    "\n",
    "- First, find the $\\lambda_{max}$ value by tuning the derivative of $\\lambda$ to be zero using the `lmbda` slider to change the wavelength.\n",
    "- Repeat this process for several temperatures.\n",
    "- A linear plot between $\\lambda_{max}$ and inverse $T$ will be made by incorporating the points into the code below.\n",
    "\n",
    "## Your task\n",
    "1. Rerun the code block below to produce the interactive plot that shows the derivative of a black body radiation curve approximated with the central difference formula. \n",
    "2. Adjust the temperature slider to any given temperature and record the value in the **`temperature_data`** structure in the [last code block](#collect_data) of the notebook.\n",
    "3. At this temperature, position the `lmbda` slider to the top of the black body peak (blue line) where the derivative is as close to zero as possible. This represents $\\lambda_{max}$. Record the value of $\\lambda_{max}$ in the **`lambda_data`** structure in the  [last code block](#collect_data) of the notebook.\n",
    "4. Repeat steps 2 & 3 at 8 different temperatures.\n",
    "5. Compare the slope to true value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='collect_data'></a>Input your series of temperatures and the corresponding lambda_max for that temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_data = np.array([]) #put a comma serpated list of the tested temperatures you explored here\n",
    "lambda_data = np.array([]) #comma seperated list of lambdamax of at each temperature\n",
    "\n",
    "pf.regression_plot(1.0/temperature_data,lambda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
